# -*- coding: utf-8 -*-
"""Finetuning_tinyllama2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1puDSRXi22oXh06TXeLqtiiDp23MPWMU6
"""

!pip install transformers datasets peft trl bitsandbytes accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

"""## Data Preprocessing"""

import pandas as pd
import json

# Load CSV (adjust encoding if needed)
csv_path = "taylor_swift_lyrics.csv"
df = pd.read_csv(csv_path, encoding="ISO-8859-1")

# Check required columns exist
assert 'track_title' in df.columns and 'lyric' in df.columns, "Missing expected columns in CSV."

# Group lyrics by track
grouped = df.groupby('track_title')['lyric'].apply(list)

# Build instruction-format JSON objects
output_data = []
for title, lyrics in grouped.items():
    for i in range(0, len(lyrics), 3):  # group every 3 lines
        lines = lyrics[i:i+3]
        if len(lines) < 2:
            continue  # skip too-short chunks
        example = {
            "instruction": "Write a Taylor Swift-style lyric:",
            "input": "",
            "output": "\n".join(line.strip() for line in lines if isinstance(line, str))
        }
        output_data.append(example)

# Save to JSONL
jsonl_path = "taylor_swift_lyrics_finetune.jsonl"
with open(jsonl_path, "w", encoding="utf-8") as f:
    for item in output_data:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"Saved {len(output_data)} examples to: {jsonl_path}")

# Load base TinyLLaMA model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True
)

# LoRA configuration
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# Load fine-tuning dataset
dataset = load_dataset("json", data_files="taylor_swift_lyrics_finetune.jsonl")

# Training args
training_args = TrainingArguments(
    output_dir="./tinyllama-lyrics-lora",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2
)

def format_example(example):
    prompt = f"### Instruction:\n{example['instruction']}\n### Response:\n{example['output']}"
    return {"text": prompt}

dataset = dataset.map(format_example)

# Optional: remove prompt tokens from loss
data_collator = DataCollatorForCompletionOnlyLM(
    tokenizer=tokenizer,
    instruction_template="### Instruction:\n{instruction}\n### Response:\n",
    response_template="{output}") # Add response_template)

# Trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    #tokenizer=tokenizer,
    data_collator=data_collator,
    args=training_args,
    #dataset_text_field="output"
)

trainer.train()

prompt = "Write a Taylor Swift-style lyric:"
input_text = f"### Instruction:\n{prompt}\n### Response:\n"

inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))